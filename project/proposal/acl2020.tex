
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{CS272 Project Pitch: Generalizing Humanitarian-Centric Topics from Twitter Crisis Data for Multilabel Classification and Tagging\\
Team: \texttt{NLTweetRelief}}

\author{Sam Showalter \\
  UC, Irvine \\  
\texttt{showalte@uci.edu} \And
Edgar Robles \\
  UC, Irvine \\  
\texttt{roblesee@uci.edu}\And
Preethi Seshadri \\
  UC, Irvine \\  
\texttt{preethis@uci.edu}}

\date{}

\begin{document}
\maketitle


\section{Project Overview}

\subsection{Problem Setup}

Swift and comprehensive response to disaster situations is crucial for maintaining safety in society. However, it can be difficult to quickly understand the full extent of a disaster, as severity may not immediately be known. With the proliferation of social media giving everyone unfettered access to the internet, disasters are often first reported on Twitter and other social, mobile-based platforms. Improvements in deep learning have enabled scientists to track, tag, and categorize Tweets such that disaster response may be swift. In particular, connecting social media information about disasters to humanitarian causes (loss of life, injury, caution, etc.) is an ongoing body of research. However, with existing datasets few researchers have framed the problem as a multilabel classification, assuming one semantic context per tweet or message. In social media platforms without character limits (Facebook, Instagram, etc.), it is both possible and likely that messages about a crisis may span multiple topics. Models that can tag text with several labels, as well as localize the information relevant to that tag, are indispensable for organizing information about a crisis for later dissemination.

Moreover, virtually no datasets exist that model crisis message classification as a multilabel objective. This is fundamentally flawed; nearly all humanitarian topics are tightly correlated in crisis messages, making organization of information difficult and topics entangled (e.g. loss of life statistics may include aid information, injury, etc.). In society, the separation of crisis events by humanitarian topic is essential as response agencies tends to naturally stratify along humanitarian needs (injury, donation, infrastructure, etc.).

Multilabel classification has been done before~\cite{schulz2014}, however, it was done using TF-IDF vectors with a neural network. Unsupervised information extraction from crisis twitter data has also been used to rank the importance of tweets~\cite{interdonato2018}. In addition, methods to decide whether tweets during crisis are relevant or not exist~\cite{kruspe2020}, as well as methods to map crises using twitter's geolocation data~\cite{middleton2014} and create reports from extracted information~\cite{corso2017}.

However, none of these approaches directly tackle the task of explicitly separating crisis information by humanitarian topic in a multilabel objective. This is important because report generation, relevance, and information extraction performance will suffer if information is not semantically clustered. Below, we propose a method that would allow us to leverage data augmentation to generalize a standard classification objective for crisis tweets into a multilabel information extraction tool applicable to text beyond social media.

\subsection{Proposed Approach}
On April 8, 2021, \texttt{HumAID} \cite{humaid2020} - the largest collection of \href{https://crisisnlp.qcri.org/humaid_dataset}{disaster related tweets} - compiled and annotated for a variety of humanitarian sub-incidents (loss of life, injury, property damage, etc.), was released. We feel this presents an unique opportunity to develop a multilabel NLP system that can also extract text relevant to each detected tag. Since each tweet in our dataset is aligned with a single humanitarian label, we intend to define a data augmentation approach protocol that samples sets of tweets (1 to k). By randomly sampling a varying number of tweets, we then will create \textit{passages} that will serve as our augmented training samples. After training this data on all disasters up to 2019, we will evaluate the system on \textit{passages} from disasters occurring from 2019 onward. There are several benefits of this data augmentation objective, including a dataset of virtually unlimited size. We ensure in our augmentation that tweets remain clustered by crisis.


We will likely make use of a transformer-based model trained on general text and may also embed the original tweets using a crisis-specific embedding model \cite{nguyen2017robust}. As part of our experimentation, we will qualitatively apply our model to non-tweet text to qualitatively examine its generalization to crisis information in news articles and other sources.

\subsection{Evaluation Plan}
For the traditional classification performance on the multilabel objective, a log-loss or cross entropy can be utilized to extract how well the model generalizes in its multilabel objective. In addition, a word error rate across the extracted messages will determine how precise and complete the textual extractions were for each label. Baseline performance on the multilabel objective can be conducted with a model that tags each word with its humanitarian topic with the ultimate goal of recovering the original independent tweets of the HumAID dataset. Once a baseline performance has been attained, we intend to manually weight the importance of different classification objectives based on their humanitarian severity (e.g. loss of life would be considered more severe than infrastructure damage). Our original model will be evaluated under this regime, and then a new system will be tuned for the weighted objective and compared to the naive baseline.

Lastly, qualitative analysis will be conducted to see how well this multilabel model adapts to non-twitter text when applied to longhand articles documenting the events of a crisis. Ideally, this multilabel crisis classifier would be able to extract and organize information from all text data sources, not just Twitter and social media. To that end, we will also tune our model pretraining to attempt to bridge the lexical gap between Tweets and more proper news correspondences.

\subsection{Potential Challenges}%
\label{sub:potential_challenges}

No members of our group have directly conducted research in Natural Language Processing, and only a few members have worked with state of the art language models (i.e. Transformers). It will take some time for us to understand the strengths and weaknesses of these SOTA models as well as how they fit to our objective. At the same time, our experiments are fairly reliant on our data augmentation protocol, which, though promising, may become the source of performance issues. Lastly, generalizing crisis data extraction to non-Twitter corpuses may results in performance dips as the lexicon's between these two domains can vary significantly. With that said, we maintain this approach is both promising as a starting point for future work and as an implementation in its own right. 

\subsection{Plan of Work}%
\label{sub:plan_of_work}

Currently, we have a basic data loader that reads and organizes our crisis Tweet information. Moreover, we have an environment that will support GPU accelerated training and, if needed, additional parallelization across machines. We hope that by the end of our proposal we can have a fairly basic, pre-trained model finetuned for our objective and producing reasonable performance on the unilabel crisis tagging objective. From there, we think that it is reasonable to expect another implementation, this time including the data augmentation protocol, to establish baseline performance on the multilabel objective. It is unclear how much additional qualitative and qualitative finetuning we will have time for after this, but ideally we would like to establish at least one qualitative result and a few model / data ablations, ultimately honing in on a tuned model.

\subsection{Computational Requirements}
Our datasets will likely fit in 10GB of RAM during experimentation and we will probably leverage GPU support via one of the ICS Computing servers available to us. Storage space for our solutions will represent less than 2 GB.



\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\appendix

\end{document}% File acl2020.tex

