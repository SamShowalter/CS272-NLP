\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zhang2010understanding}
\citation{luhn1957statistical}
\citation{jones1972statistical}
\newlabel{sec:supervised_learning}{{2}{2}{Supervised Learning}{section.2}{}}
\newlabel{sub:feature_engineering}{{2.1}{2}{Feature Engineering}{subsection.2.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:stopwords}{{1}{2}{Token frequencies in training data before and after stopwords removed from the dataset\relax }{figure.caption.1}{}}
\newlabel{fig:imgs/token_filter}{{2}{2}{Vocabulary size as successive filters applied to input corpus. From left to right, the strings are regex separated, then case and punctuation is resolved, with final filters trimming rare tokens\relax }{figure.caption.2}{}}
\citation{turney2010frequency}
\citation{mikolov2013distributed}
\citation{lilleberg2015support}
\newlabel{sub:dimensionality_reduction}{{2.2}{3}{Dimensionality Reduction}{subsection.2.2}{}}
\newlabel{sub:model_tuning}{{2.3}{3}{Model Tuning}{subsection.2.3}{}}
\newlabel{fig:tsvd}{{3}{3}{Classification performance applied to best feature selection after Truncated SVD applied with varying numbers of components\relax }{figure.caption.4}{}}
\newlabel{sec:semi_supervised_learning_with_word_embeddings}{{3}{3}{Semi-supervised Learning with Word Embeddings}{section.3}{}}
\newlabel{sub:word2vec_for_text_classification}{{3.1}{3}{Word2Vec for Text Classification}{subsection.3.1}{}}
\citation{turney2010frequency}
\newlabel{fig:imgs/w2v_pca}{{4}{4}{Visualization of Word2Vec embeddings with PCA with a selection of common political terms. Embeddings appear to capture semantic meaning\relax }{figure.caption.6}{}}
\newlabel{subseq:experimental_results_and_discussion}{{3.2}{4}{Experimental Results and Discussion}{subsection.3.2}{}}
\newlabel{fig:imgs/PCA_w2v_Obama}{{5}{4}{Visualization of political candidate embedding profiles as defined by their speech embeddings. Little separation can be cleaned, likely because order is not considered in embedding creation and candidates discuss similar topics\relax }{figure.caption.7}{}}
\citation{pennington2014glove}
\citation{hochreiter1997long}
\citation{vaswani2017attention}
\bibdata{custom}
\bibcite{hochreiter1997long}{{1}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\newlabel{fig:imgs/w2v_classification}{{6}{5}{Performance with concatenated Word2Vec document embeddings and different fractions of unlabeled documents and SAG, LBFGS gradient solvers\relax }{figure.caption.8}{}}
\newlabel{sec:conclusion_and_further_exploration}{{4}{5}{Conclusions and Further Exploration}{section.4}{}}
\bibcite{jones1972statistical}{{2}{1972}{{Jones}}{{}}}
\bibcite{lilleberg2015support}{{3}{2015}{{Lilleberg et~al.}}{{Lilleberg, Zhu, and Zhang}}}
\bibcite{luhn1957statistical}{{4}{1957}{{Luhn}}{{}}}
\bibcite{mikolov2013distributed}{{5}{2013}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{pennington2014glove}{{6}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{turney2010frequency}{{7}{2010}{{Turney and Pantel}}{{}}}
\bibcite{vaswani2017attention}{{8}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{zhang2010understanding}{{9}{2010}{{Zhang et~al.}}{{Zhang, Jin, and Zhou}}}
\bibstyle{acl_natbib}
