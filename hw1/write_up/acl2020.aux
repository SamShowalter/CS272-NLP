\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{pennington2014glove}
\bibdata{custom}
\bibcite{pennington2014glove}{{1}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\newlabel{sec:supervised_learning}{{2}{1}{Supervised Learning}{section.2}{}}
\newlabel{sub:feature_engineering}{{2.1}{1}{Feature Engineering}{subsection.2.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:stopwords}{{1}{1}{Token frequencies in training data before and after stopwords removed from the dataset\relax }{figure.caption.1}{}}
\newlabel{fig:imgs/token_filter}{{2}{1}{Vocabulary size as successive filters applied to input corpus. From left to right, the strings are regex separated, then case and punctuation is resolved, with final filters trimming rare tokens\relax }{figure.caption.2}{}}
\newlabel{sub:dimensionality_reduction}{{2.2}{1}{Dimensionality Reduction}{subsection.2.2}{}}
\bibstyle{acl_natbib}
\newlabel{fig:tsvd}{{3}{2}{Classification performance applied to best feature selection after Truncated SVD applied with varying numbers of components\relax }{figure.caption.4}{}}
\newlabel{fig:imgs/w2v_pca}{{4}{2}{Visualization of Word2Vec embeddings with PCA with a selection of common political terms. Embeddings appear to capture some level of semantic meaning\relax }{figure.caption.6}{}}
\newlabel{fig:imgs/w2v_classification}{{5}{2}{Classification performance of Word2Vec document embeddings with and without original SVD features concatenated. The x-axis represents the fraction of unlabeled documents leveraged to make the Word2Vec model\relax }{figure.caption.7}{}}
\newlabel{fig:imgs/PCA_w2v_Obama}{{6}{2}{Visualization of political candidate embedding profiles as defined by their speech embeddings. Little separation can be cleaned, likely because order is not considered in embedding creation\relax }{figure.caption.8}{}}
\newlabel{sub:model_tuning}{{2.3}{2}{Model Tuning}{subsection.2.3}{}}
\newlabel{sec:semi_supervised_learning_with_word_embeddings}{{3}{2}{Semi-supervised Learning with Word Embeddings}{section.3}{}}
\newlabel{sub:word2vec_for_text_classification}{{3.1}{2}{Word2Vec for Text Classification}{subsection.3.1}{}}
\newlabel{sec:experimental_results_and_discussion}{{4}{2}{Experimental Results and Discussion}{section.4}{}}
\newlabel{sec:conclusion_and_further_exploration}{{5}{2}{Conclusions and Further Exploration}{section.5}{}}
