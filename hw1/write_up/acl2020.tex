
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{float}
\usepackage{placeins}
\usepackage{booktabs}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Hw1: Presidential Speech Classification with Word Embeddings}

\author{Sam Showalter \\
  University of California, Irvine \ (showalte) \\  
\texttt{showalte@uci.edu}} 

\date{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\begin{abstract}
  Text classification in model-restrictive settings is heavily reliant on predictive features to ensure adequate performance. In this case, our task is to predict a presidential candidate by looking at a small text snippet of varying length taken from a transcription of one their speeches. This analysis explores several methods of text tokenization, feature engineering, dimensionality reduction, and model loss tuning. Starting with simple dataset exploration, our experiments extended to attempts at encoding semantic meaning into the naive bag-of-words representations with Truncated Singular Value Decomposition and Word2Vec embeddings.
  Moreover, this experimentation was also applied to the model itself. Since the input features from the presidential speeches are very high-dimensional, different regularization methods and gradient solvers were applied to improve learning and generalization. After each ablation, performance on a held-out validation set improved, and the best configuration from the experiment was passed to the next and built upon progressively. Overall, we achieved a boost of roughly 2.5\% over the baseline.
  In addition, we further improved our classification performance by pretraining an embedding model on unlabeled data with Word2Vec, correlating words based on their meaning. Though alone these word embeddings did not exceed the supervised performance, when integrated the original set of supervised features performance improved slightly. However, the embeddings did exhibit a few failure modes that could be tracked back to the structure of the unlabeled data.  
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Text classification can be accomplished with a variety of implementations. Among the simplest and most popular is the bag-of-words framework \cite{zhang2010understanding}. In this setting, a vocabulary is formed by tokenizing the input text and then creating a unique index for every distinct token. Then, one-hot vectors of each token are generated and a document's feature vector is generated as the sum or the logical-OR of the tokens within it.

In general, feature vectors generated in this way tend to be high-dimensional. To combat this curse of dimensionality in modeling, the vocabulary can be shrunk by several methods, including resolving the case of all characters, removing non-alphanumeric character, expanding contractions, lemmatization, and removing words that tend to carry little semantic meaning (\textit{stopwords}). Even with these reductions, which at times may be substantial, the resulting feature vectors tend to still be quite large. Further explicit reductions can come from recognizing that tokens that occur extremely rarely across a corpus likely carry little predictive value and may be ignored. This practice should be conducted with caution; while tokens that occur very rarely (e.g. once) may not be predictive, tokens that occur slightly more often (e.g. in 1\% of documents) may be very predictive. The appropriate frequency threshold for ignoring rare tokens varies by application and may need to be discovered empirically. However, once a suitable feature engineering protocol has been defined, the only other tuning possible is incorporating unlabeled data and ablations with training the model itself.

In subsequent sections, we outline our approach to feature engineering, supervised learning, and incorporating unlabeled data. Our main experimental contributions can be defined as follows:
\begin{enumerate}
  \item We extensively explore feature engineering pipelines, including tokenization creation and resolution methods.
    \item We explore dimensionality reduction techniques with and without the use of unlabeled data with Truncated SVD and Word2Vec.
      \item We regularize our model, Logistic Regression, in several different ways as well as apply different gradient based solvers for the optimization.
\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Supervised Learning Grid Search
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Supervised Learning}%
\label{sec:supervised_learning}

In this section, we discuss methods of improving classification performance without making use ofunlabeled data. The following sections are separated by feature and model-based exploration.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Feature Engineering
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Feature Engineering}%
\label{sub:feature_engineering}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To begin our analysis, a few small tests on our input data's vocabulary size was conducted. As expected, the tokens found with a simple regular expression (\textit{regex})  separator exhibited an exponential decrease in frequency from most common to least. Fortunately, we were able to drop the differential between the most common token and the least by an order of magnitude simply be removing stopwords, as shown in Figure \ref{fig:stopwords}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stopwords token filtering figure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=1\linewidth]{imgs/stopwords.png}
  \caption{Token frequencies in training data before and after 
  stopwords removed from the dataset}%
  \label{fig:stopwords}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While this simple experiment gave us insight into the distribution of the token frequencies, we still needed to better understand the influence different preprocessing operations had on vocabulary size. As mentioned in the introduction, several techniques exist to reduce vocabulary size while largely preserving the semantic meaning of the input. A subset of these were applied to the input data, and the results are displayed in Figure \ref{fig:imgs/token_filter}. After regex separation, several additional filters are applied to the training data, with the steepest drop in cardinality occurring from the limitation of minimum document frequency to two. This implies that many tokens only occur within a single document and likely only once in the training dataset. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Token dimensionality exploration
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=1\linewidth]{imgs/token_filter.png}
  \caption{Vocabulary size as successive filters applied to input corpus. 
    From left to right, the strings are regex separated, then case and
  punctuation is resolved, with final filters trimming rare tokens}
  \label{fig:imgs/token_filter}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To test different theories about the impact of feature engineering, we ran a grid search of several different preprocessing methods on a standard implementation of Logistic Regression (lbfgs solver with L2 loss). Specifically, we leveraged two different tokenization strategies: a conservative regex tokenization from Scikit-Learn's \texttt{CountVectorizer} and a more sophisticated method from Python's \texttt{NLTK Toolkit}. In addition, we also resolved the case of characters and removed punctuation as additional ablations. Finally, lemmatization, or deconjugating words, was applied as was the removal of stopwords. Once these were completed, vectors were generated from the tokens with two methods: the traditional bag-of-words \cite {luhn1957statistical} count vectorization, and tf-idf vectorization. Tf-idf, or term-frequency inverse document frequency \cite{jones1972statistical}, extends on the bag of words notion by upweighting terms that do not appear in many documents. That is, if a term occurs rarely across documents, it should be upweighted as it may prove more predictive.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Feature engineering grid search table
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
\begin{table}[h]
    \centering
    \caption{Feature Engineering Grid Search}
    \begin{tabular}{
    	l
        l
        l
        l
        l
        l
        }
        \toprule
        \multicolumn{2}{c}{} &
        \multicolumn{4}{c}{Token Filters}\\
        \cmidrule(lr){3-6} 
        {Token}& {Tok.} &{None} &{Case+}&{Lem.}& {Stop}\\
        {Engine} & {Sep.} && {Punct.}&&{Word}\\
        \midrule
      CVect & nltk & 0.399& \textbf{0.435}& \textbf{0.435}& 0.401\\
        CVect & reg& 0.396& 0.413&0.413& 0.399\\
        Tf-Idf & nltk & 0.382&  0.382 &0.382 & 0.386\\
        Tf-Idf & reg & 0.360& 0.374 & 0.374& 0.381\\
        \bottomrule
    \end{tabular}
\end{table}
\FloatBarrier
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Displayed in Table 1, several findings surprised us. In particular, the removal of stopwords tended to hinder performance or at least not improve it substantially. Tf-idf vectorization, often thought as an improvement to standard counts, actually performed worse in all cases. These two phenomena may be due to the fact that the filler words a candidate uses are somewhat unique to their speaking style and therefore predictive. Less surprisingly, nltk's more sophisticated tokenization engine outperformed the more conservative regex counterpart, perhaps slightly due to how it handles contractions (breaking \texttt{can't} into \texttt{ca} + \texttt{n't} to separate the contraction from the root word). The resolution of case, punctuation, and lemmatization also tended to lead to improvements, though at times this boost was not substantial, perhaps due to the fundamential difficulty of the task and the capacity of our model. Fortunately, this grid search did identify a new method of featurizing the input corpus that led to a boost of 2.7\% and 2.1\% over the validation and testing baselines, respectively. We used the preprocessing protocol that achieved the highest score with the lowest dimensionality for out following analyses.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Dimensionality Reduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dimensionality Reduction}%
\label{sub:dimensionality_reduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While one method of dimensionality reduction in NLP focuses on trimming out words with little predictive value, the results of the feature engineering study led us to believe this would not be as intuitive as previously thought. Therefore, we leveraged Truncated SVD, a matrix decomposition method amenable to sparse data, to generate more semantic features without needing explicitly remove tokens. Below, we ran Truncated SVD across the features generated previously for an increasing number of components. With only 2500 components, roughly a third of the original set, we were able to exceed our previous performance on the held-out validation set (denoted with red dotted line). For a separate trial, features were normalized before SVD, drastically hurting performance. We posit this is due to the sparsity of the data, where normalization is skewed by the immense presence of zero-terms.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Dimensionality Rediction Figure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=1\linewidth]{imgs/tsvd.png}
  \caption{Classification performance applied to best feature
  selection after Truncated SVD applied with varying numbers of 
 components}%
  \label{fig:tsvd}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Model Ablation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model Tuning}%
\label{sub:model_tuning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

With a smaller set of features and boosted performance on the baseline model, we turned our attention to model tuning directly. Concerned with the effect of dimensionality on the Logistic Regression, we explored many regularization strategies including L1 and L2 loss. For comparison, we also ran a trial where no regularization was applied.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Model Grid Search Plot
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
    \centering
    \caption{Model Tuning: Logistic Regression}
    \begin{tabular}{
    	l
        l
        l
        l
        l
        l
        }
        \toprule
        \multicolumn{1}{c}{} &
        \multicolumn{5}{c}{Logistic Reg. Solver}\\
        \cmidrule(lr){2-6} 
        {Reg.}& {lbfgs} &{lib-} &{newton}&{saga}& {sag}\\
        {Pen.} & {} &{linear}& {}&&{}\\
        \midrule
        - & 0.399 & - & 0.401 & \textbf{0.454} & 0.447\\
        L1 & - & 0.423 &  -&0.428& -\\
        L2 & 0.436& 0.432& 0.448& 0.447& \textbf{0.450}\\
        \bottomrule
    \end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In general, L1 loss appeared to be a inferior regularization of the input data, surprising since inducing sparsity on a high-dimensional dataset would improve the predictive power of the model \cite{turney2010frequency}. Moreover, the non-regularized model performed well on the validation set but hurt performance on the testing set, implying the non-regularized optima was not as robust as its regularized counterparts. Affirming this notion, the best L2 loss configuration boosted our performance on our testing submission. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Semi-Supervised Learning 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Semi-supervised Learning with Word Embeddings}%
\label{sec:semi_supervised_learning_with_word_embeddings}

With our supervised model and data tuned, this section explores methods of incorporating unlabeled data to boost performance. In particular, we explore Word2Vec 
embeddings to encode semantic meaning between tokens.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Word2Vec
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Word2Vec for Text Classification}%
\label{sub:word2vec_for_text_classification}

Word2Vec \cite{mikolov2013distributed} is a linear model implementation what seeks to embed words that co-occur together, assuming word proximity implies shared meaning \cite{lilleberg2015support} . In our implementation, we use a window of 5, meaning the two words to the left and right of a word are considered during the embedding process. More interesting, this method implicitly downsamples more frequent words, effectively handling the influence of stopwords. As shown in Figure \ref{fig:imgs/w2v_pca}, a Principal Component Analysis (PCA) decomposition of a selection of word embeddings, candidates tend to be co-located together, as do words connoted with conflict or health. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Word2Vec PCA example
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=1\linewidth]{imgs/w2v_pca.png}
  \caption{Visualization of Word2Vec embeddings with PCA with a selection 
  of common political terms. Embeddings appear to capture semantic meaning}%
  \label{fig:imgs/w2v_pca}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We chose to utilize word embedding models over the alternatives because we felt it would better generalize similarity in meaning between tokens and boost performance. Other implementations considered included dataset augmentation with unlabeled data. First, a model is trained with labeled data, then it is iteratively used to label unlabeled instances. If these predictions meet certain confidence criteria, then the sample is added to the training set and the process continues until performance ceases to improve. Though a promising option, we were concerned the approach would cause the trained model to diverge under its own, highly imperfect predictions, leading to poorer performance overall. Empricially validating this assumption is one promising avenue for future research.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results and Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Results and Discussion}%
\label{subseq:experimental_results_and_discussion}

Given a large corpus of 43,000 unlabeled speech snippets, we then trained several Word2Vec modelswith different fractions of that data. The embedded dimensionality of these words is 300, a reduction of the previous vocabulary size by an order of magnitude. To generalize these word embeddings to documents, the word embeddings in the document are averaged, an order agnostic but somewhat performant approach \cite{turney2010frequency}. Taken alone, these document embeddings were not able to come close to even the initial baseline performance, but increasing the document fraction of the Word2Vec model did improve performance in almost all cases, peaking at 0.34.

We think one potential reason the Word2Vec embeddings alone did not boost or even meet previous performance is due to its averaging of embeddings. Since documents are composed of token sequences of varying length, it is typically not useful to sum the embeddings. Averaging these embeddings overcomes this issue, but also comes with the cost that meaning is obscured across many tokens. Since many presidential candidates speak on similar topics, there may only be subtle differences in the composite embeddings. This is verified by looking at the embeddings of documents taken from the primary speeches of Barack Obama and Hillary Clinton, the two most common classes in the dataset, displayed in Figure \ref{fig:imgs/PCA_w2v_Obama}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Word2Vec Candidate Embeddings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=1\linewidth]{imgs/PCA_w2v_Obama.png}
  \caption{Visualization of political candidate embedding profiles
  as defined by their speech embeddings. Little separation can be cleaned,
likely because order is not considered in embedding creation and candidates discuss similar topics}%
  \label{fig:imgs/PCA_w2v_Obama}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Examining this phenomena further, we find that among the 100 most common phrases tied to the tokens \texttt{obama} and \texttt{clinton}, only half of them were unique between them. Indeed, much of what these candidates discuss is interrelated, as would be expected in a presidential race. With that said, there are still several defining features the embedding model was able to discern. Notably, the model tied the prases \texttt{tony}, \texttt{foreigner}, and \texttt{president} strongly to Obama but not Clinton. \texttt{tony} refers to Tony Rezko, an Obama fundraiser who was sentenced to prison, while the others features, namely comments about his citizenship by other candidates and his ascent to president. By contrast, words such as \texttt{bubba} allude to her campaign to win the vote of white men in the southern United States.

As shown, there is considerable overlap in what the two candidates discussed, as described by their document embeddings mapped down to two dimensions. Thus, to differentiate candidates more effectively, these 300-dimensional embeddings were concatenated together with the SVD features of the previous analysis. By incorporating embeddings from models with different amounts of labeled data, we explored performance changes with our tuned model and this concatenated feature set. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Word2Vec Performance
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=1\linewidth]{imgs/w2v_classification.png}
  \caption{Performance with concatenated Word2Vec document embeddings and different fractions of unlabeled documents and SAG, LBFGS gradient solvers}
  \label{fig:imgs/w2v_classification}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Visualized in Figure \ref{fig:imgs/w2v_classification}, Word2Vec models that were trained on 60\%of the unlabeled data proved optimal for downstream classification. It is surprising to see that 
performance decreased as the fraction of unlabeled data utilized exceeded 60\%, and may be due to stochasticity in training or other spurious modes of interaction between our original and embedded data representations. Regardless, The performance on our validation set was again improved, though unfortunately this boost in performance again did not translate to the validation set, though it exceeded the baseline.

One reason for this validation-test performance mismatch may be due to the fact that the validation set is 10 times smaller than the training set and 100 times smaller than the testing set. Future experimentation could benefit from a more balanced split. We can also explore performance more closely with a few examples. In one document, the phrase \texttt{when Bill asks what time it is } was first predicted to be said by Obama, as the word \texttt{Bill} (Bill Clinton) was conflated with legistlation. The Word2Vec model disambiguated this issue and correctly predicted Hillary Clinton, who was speaking of her husband. Unfortunately, this extra context was not always helpful. The phrase \texttt{the comatose stance "Re-elect Obama"}, stated in criticism of Democrats by Gingrich, was misinterpreted by the additional context of democrats supporting Obama in his bid for re-election in 2012. Instead of predicting Gingrich, the model now associated the phrase with Hillary Clinton, a vocal proponent of the democratic party. Additional context does not always improve classification.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Further Exploration}%
\label{sec:conclusion_and_further_exploration}

Through a sequence of grid search explorations, model tuning, and contextual word embeddings, we were able to improve our classifier beyond the existing baseline. We feel that this can be credited to our consideration of data tokenization, feature engineering, and principled methods for dimensionality reduction. However, the additional context we incorporated was not universally beneficial. Some defining traits of this project include the restriction to a Logistic Regression classifier, a highly imbalanced split between train, validation, and test data, as well as a difficult classification objective that utilizes a small snippet of a candidate's speech. Because of these defining traits, we were only able to boost performance in this classifier a few percent. With that said, if restrictions regarding models and external data were relaxed, we feel we could boost our performance far higher than the improvement noted in this report.

Some potential avenues for future research within the stated constraints of the problem include alternative word embedding strategies such as GloVE \cite{pennington2014glove} may boost performance by better capturing context between words. In addition, designing a method to incorporate word order would likely improve the expressivity of our features. This could be a simple as incorporating bigrams or training a sequence \cite{hochreiter1997long} or self-attention \cite{vaswani2017attention} model to keep a memory statistic of the sequence information.External sources of data such as a pretrained embedding model on a larger corpus could be useful as well and fine tuned for our specific objective.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Statement of Collaboration
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statement of Collaboration}
For this project I utilized a variety of resources. These include articles read from google scholar, API documentation for different NLP packages,  and office hours with Dr. Singh. Beyond these resources, I completed this project in isolation and the content within this report is my own.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\bibliography{custom}
\bibliographystyle{acl_natbib}

\appendix




\end{document}% File acl2020.tex

