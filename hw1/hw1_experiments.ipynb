{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recovered-finance",
   "metadata": {},
   "source": [
    "# Experimentation for nlp hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "maritime-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from preprocessing import *\n",
    "from speech import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from supervised_experiments import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "fname = \"speech.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-guyana",
   "metadata": {},
   "source": [
    "### General submission code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "iraqi-medium",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| '-- train data'\n",
      "ic| member.name: 'train.tsv'\n",
      "ic| len(self.train_data): 4370\n",
      "ic| '-- val data'\n",
      "ic| member.name: 'dev.tsv'\n",
      "ic| len(self.val_data): 414\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "ic| val_acc: 0.4251207729468599\n",
      "ic| 'Reading unlabeled data'\n",
      "ic| unlabeled.X.shape: (43342, 2500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pred file\n"
     ]
    }
   ],
   "source": [
    "data = Data(fname)\n",
    "data.preprocess(CountVectorizer(tokenizer = LemmaTokenizer(), \n",
    "                                stop_words = \"english\", min_df = 2),\n",
    "                svd = TruncatedSVD(n_components = 2500))\n",
    "clf = LogisticRegression(solver = \"saga\", penalty = 'l2')\n",
    "clf.fit(data.train_x, data.train_y)\n",
    "\n",
    "# Train performance\n",
    "preds = clf.predict(data.val_x)\n",
    "train_acc = accuracy_score(clf.predict(data.train_x), data.train_y)\n",
    "\n",
    "#Val performance\n",
    "val_acc =  accuracy_score(data.val_y, preds)\n",
    "ic(val_acc)\n",
    "\n",
    "\n",
    "ic(\"Reading unlabeled data\")\n",
    "unlabeled = read_unlabeled(\"data/\" + fname, data)\n",
    "print(\"Writing pred file\")\n",
    "write_pred_kaggle_file(unlabeled, clf, \"speech-pred.csv\", data)\n",
    "\n",
    "# # # You can't run this since you do not have the true labels\n",
    "# # # ic \"Writing gold file\"\n",
    "# # # write_gold_kaggle_file(\"data/speech-unlabeled.tsv\", \"data/speech-gold.csv\")\n",
    "# # # w:rite_basic_kaggle_file(\"data/speech-unlabeled.tsv\", \"data/speech-basic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-certification",
   "metadata": {},
   "source": [
    "## Supervised Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-breakfast",
   "metadata": {},
   "source": [
    "### Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dutch-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| '-- train data'\n",
      "ic| member.name: 'train.tsv'\n",
      "ic| len(self.train_data): 4370\n",
      "ic| '-- val data'\n",
      "ic| member.name: 'dev.tsv'\n",
      "ic| len(self.val_data): 414\n"
     ]
    }
   ],
   "source": [
    "fname = \"speech.tar.gz\"\n",
    "preprocessors = [\n",
    "    CountVectorizer(),\n",
    "    CountVectorizer(stop_words=\"english\"),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer()),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer(), min_df = 2),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer(), min_df = 3),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer(), min_df = 4),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer(), min_df = 5),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer(), min_df = 6),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer(), min_df = 7),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer(), min_df = 8),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer(), min_df = 9),\n",
    "    CountVectorizer(stop_words=\"english\", tokenizer = LemmaTokenizer(), min_df = 10),\n",
    "]\n",
    "\n",
    "d = dimensionality_exploration(fname, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "regular-radius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7916, 7645, 7426, 3641, 2561, 2001, 1654, 1418, 1247, 1125, 993, 895]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rough-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"speech.tar.gz\"\n",
    "\n",
    "feat_list = {\n",
    "    \"cv\":CountVectorizer(),\n",
    "    \"cv_lemma\": CountVectorizer(tokenizer = LemmaTokenizer()),\n",
    "    \"cv_stopw\": CountVectorizer(stop_words = \"english\"),\n",
    "    \"cv_lemma_stopw\":CountVectorizer(stop_words = \"english\", tokenizer = LemmaTokenizer()),\n",
    "    \"tfidf\":TfidfVectorizer(),\n",
    "    \"tfidf_lemma\": TfidfVectorizer(tokenizer = LemmaTokenizer()),\n",
    "    \"tfidf_stopw\": TfidfVectorizer(stop_words = \"english\"),\n",
    "    \"tfidf_lemma_stopw\":TfidfVectorizer(stop_words = \"english\", tokenizer = LemmaTokenizer()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-carroll",
   "metadata": {},
   "source": [
    "### Feature ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "historic-colon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| '-- train data'\n",
      "ic| member.name: 'train.tsv'\n",
      "ic| len(self.train_data): 4370\n",
      "ic| '-- val data'\n",
      "ic| member.name: 'dev.tsv'\n",
      "ic| len(self.val_data): 414\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "a1 = feature_ablation(fname, feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "utility-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 0.41304347826086957,\n",
       " 'cv_lemma': 0.43719806763285024,\n",
       " 'cv_stopw': 0.39855072463768115,\n",
       " 'cv_lemma_stopw': 0.4227053140096618,\n",
       " 'tfidf': 0.3743961352657005,\n",
       " 'tfidf_lemma': 0.38164251207729466,\n",
       " 'tfidf_stopw': 0.38164251207729466,\n",
       " 'tfidf_lemma_stopw': 0.3743961352657005}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-confidence",
   "metadata": {},
   "source": [
    "### Dimensionality Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "indie-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| '-- train data'\n",
      "ic| member.name: 'train.tsv'\n",
      "ic| len(self.train_data): 4370\n",
      "ic| '-- val data'\n",
      "ic| member.name: 'dev.tsv'\n",
      "ic| len(self.val_data): 414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4370, 7689)\n",
      "(414, 7689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "data = Data(fname)\n",
    "data.preprocess(feat_list[\"cv_lemma\"], norm = True)\n",
    "print(data.train_x.shape)\n",
    "print(data.val_x.shape)\n",
    "# sys.exit(1)\n",
    "comp_list = [100,500, 1000, 1500, 2000, 2500, 3000, 4000, 5000]\n",
    "a2 = dimensionality_ablation(data, comp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "popular-christmas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: 0.2777777777777778,\n",
       " 500: 0.32608695652173914,\n",
       " 1000: 0.3357487922705314,\n",
       " 1500: 0.33816425120772947,\n",
       " 2000: 0.33816425120772947,\n",
       " 2500: 0.33816425120772947,\n",
       " 3000: 0.34299516908212563,\n",
       " 4000: 0.34057971014492755,\n",
       " 5000: 0.33816425120772947}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "composed-albuquerque",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| '-- train data'\n",
      "ic| member.name: 'train.tsv'\n",
      "ic| len(self.train_data): 4370\n",
      "ic| '-- val data'\n",
      "ic| member.name: 'dev.tsv'\n",
      "ic| len(self.val_data): 414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4370, 7689)\n",
      "(414, 7689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/showalte/.conda/envs/nlpenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "data = Data(fname)\n",
    "data.preprocess(feat_list[\"cv_lemma\"], norm = False)\n",
    "print(data.train_x.shape)\n",
    "print(data.val_x.shape)\n",
    "# sys.exit(1)\n",
    "comp_list = [100,500, 1000, 1500, 2000, 2500, 3000, 4000, 5000]\n",
    "a2_2 = dimensionality_ablation(data, comp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "silent-enlargement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: 0.30434782608695654,\n",
       " 500: 0.37922705314009664,\n",
       " 1000: 0.4106280193236715,\n",
       " 1500: 0.4227053140096618,\n",
       " 2000: 0.43719806763285024,\n",
       " 2500: 0.4444444444444444,\n",
       " 3000: 0.4396135265700483,\n",
       " 4000: 0.4323671497584541,\n",
       " 5000: 0.43719806763285024}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-broadcasting",
   "metadata": {},
   "source": [
    "### Model ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "armed-water",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7fb272fe8c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cv_lemma_stopw\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Data' is not defined"
     ]
    }
   ],
   "source": [
    "data = Data(fname)\n",
    "data.preprocess(feat_list[\"cv_lemma\"], norm = False)\n",
    "svd = TruncatedSVD(n_components = 2500)\n",
    "data.train_x = svd.fit_transform(data.train_x)\n",
    "data.val_x = svd.transform(data.val_x)\n",
    "solvers = [\"lbfgs\", \"liblinear\", \"saga\", \"newton-cg\", \"sag\"]\n",
    "penalties = [\"l1\", \"l2\", 'none']\n",
    "a3 = solver_pen_ablation(data.train_x, data.train_y, data.val_x, data.val_y, solvers, penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "joined-composer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': {'lbfgs': '-',\n",
       "  'liblinear': 0.42995169082125606,\n",
       "  'saga': 0.43478260869565216,\n",
       "  'newton-cg': '-',\n",
       "  'sag': '-'},\n",
       " 'l2': {'lbfgs': 0.43478260869565216,\n",
       "  'liblinear': 0.4323671497584541,\n",
       "  'saga': 0.4420289855072464,\n",
       "  'newton-cg': 0.4420289855072464,\n",
       "  'sag': 0.4420289855072464},\n",
       " 'none': {'lbfgs': 0.4057971014492754,\n",
       "  'liblinear': '-',\n",
       "  'saga': 0.45652173913043476,\n",
       "  'newton-cg': 0.40096618357487923,\n",
       "  'sag': 0.4396135265700483}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-aaron",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-radical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-species",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "antique-jewel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'Reading data'\n",
      "ic| '-- train data'\n",
      "ic| member.name: 'train.tsv'\n",
      "ic| len(self.train_data): 4370\n",
      "ic| '-- val data'\n",
      "ic| member.name: 'dev.tsv'\n",
      "ic| len(self.val_data): 414\n"
     ]
    }
   ],
   "source": [
    "ic(\"Reading data\")\n",
    "tarfname = \"speech.tar.gz\"\n",
    "speech = Data(tarfname)\n",
    "speech.preprocess()\n",
    "\n",
    "# train_extra_data = np.zeros((len(speech.train_data), 4))\n",
    "# val_extra_data = np.zeros((len(speech.val_data), 4))\n",
    "# lkp_dict = {\"-\":0,\n",
    "#             \".\":1,\n",
    "#            \",\":2,\n",
    "#            \";\":3,\n",
    "#            }\n",
    "\n",
    "# for i, sentence in tqdm(enumerate(speech.train_data)):\n",
    "#     for char in sentence.decode(\"utf-8\"):\n",
    "#         if char in lkp_dict:\n",
    "#             train_extra_data[i,lkp_dict[char]] += 1\n",
    "\n",
    "# print(\"Validation\")\n",
    "# for i, sentence in tqdm(enumerate(speech.val_data)):\n",
    "#     for char in sentence.decode(\"utf-8\"):\n",
    "#         if char in lkp_dict:\n",
    "#             val_extra_data[i,lkp_dict[char]] += 1\n",
    "            \n",
    "# print(speech.train_x.shape)\n",
    "\n",
    "# speech.train_x = scipy.sparse.hstack((speech.train_x, csr_matrix(train_extra_data)))\n",
    "# speech.val_x = scipy.sparse.hstack((speech.val_x, csr_matrix(val_extra_data)))\n",
    "# print(speech.train_x.shape)\n",
    "# print(speech.val_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "through-correction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4370, 7426)\n"
     ]
    }
   ],
   "source": [
    "print(speech.train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"val_extra_data.pkl\", \"wb\") as file:\n",
    "    pkl.dump(val_extra_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-immunology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(range(len(vocabulary_sorted)), [item[1] for item in vocabulary_sorted])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.count_vect = CountVectorizer(stop_words = \"english\")#, tokenizer = LemmaTokenizer())\n",
    "matrix = speech.count_vect.fit_transform(speech.train_data)\n",
    "print(matrix.shape)\n",
    "freqs = zip(speech.count_vect.get_feature_names(), matrix.sum(axis=0).tolist()[0])    \n",
    "# sort from largest to smallest\n",
    "vocabulary_sorted = sorted(freqs, key=lambda x: -x[1])\n",
    "print(type(vocabulary_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer as lemmatizer\n",
    "lemmatizer = lemmatizer()\n",
    "voc_l = [lemmatizer.lemmatize(i[0]) for i in vocabulary_sorted]\n",
    "print(len(list(set(voc_l))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(range(len(vocabulary_sorted)), [item[1] for item in vocabulary_sorted])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-elder",
   "metadata": {},
   "source": [
    "## Unsupervised experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "linear-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_unlabeled(\"data/\" + \n",
    "                      fname, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "embedded-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43342\n",
      "b'must always seek to protect our national security by aggressively gathering intelligence in accordance with proven methods. Yet we cannot'\n"
     ]
    }
   ],
   "source": [
    "print(len(data.data))\n",
    "print(data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "declared-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CountVectorizer(stop_words = \"english\",\n",
    "#                             tokenizer = LemmaTokenizer(),\n",
    "                           )\n",
    "                            \n",
    "def sentence_parser(sentences,tokenizer, lemmatizer):\n",
    "    res_sentences = []\n",
    "    tokenizer = tokenizer.build_tokenizer()\n",
    "    \n",
    "    for s in tqdm(sentences):\n",
    "        s = tokenizer(s.decode('utf-8'))\n",
    "        res_sentences.append(\n",
    "        [lemmatizer.lemmatize(w).lower() for w in s])\n",
    "#     print(sentences)\n",
    "    return res_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "indirect-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43342/43342 [00:04<00:00, 9368.89it/s]\n"
     ]
    }
   ],
   "source": [
    "data.parsed_unlabeled_data = sentence_parser(data.data, tokenizer, WordNetLemmatizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "excellent-housing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| '-- train data'\n",
      "ic| member.name: 'train.tsv'\n",
      "ic| len(self.train_data): 4370\n",
      "ic| '-- val data'\n",
      "ic| member.name: 'dev.tsv'\n",
      "ic| len(self.val_data): 414\n",
      "100%|██████████| 4370/4370 [00:00<00:00, 10393.85it/s]\n"
     ]
    }
   ],
   "source": [
    "labeled_data = Data(fname)\n",
    "data.parsed_labeled_data = sentence_parser(labeled_data.train_data, \n",
    "                                           tokenizer, \n",
    "                                           WordNetLemmatizer())\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-barcelona",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-sitting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caroline-cannon",
   "metadata": {},
   "source": [
    "### Vectorize input in preparation for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-hungarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-anniversary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-terrain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-public",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.txt'\n",
    "word2vec_output_file = 'word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "metallic-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "latin-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "buried-twenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['dog'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-sheriff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
